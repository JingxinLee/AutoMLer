   survived     sex   age  ...     deck  embark_town  alone
0         0    male  22.0  ...  unknown  Southampton      n
1         1  female  38.0  ...        C    Cherbourg      n
2         1  female  26.0  ...  unknown  Southampton      y
3         1  female  35.0  ...        C  Southampton      n
4         0    male  28.0  ...  unknown   Queenstown      y

[5 rows x 10 columns]
count    627.000000
mean       0.387560
std        0.487582
min        0.000000
25%        0.000000
50%        0.000000
75%        1.000000
max        1.000000
Name: survived, dtype: float64
0    0
1    0
2    1
3    1
4    0
Name: survived, dtype: int64
{'accuracy': 0.7765151515151515, 'balanced_accuracy': 0.7484848484848485, 'mcc': 0.513369721923206, 'roc_auc': 0.8426385062748698, 'f1': 0.6810810810810811, 'precision': 0.7325581395348837, 'recall': 0.6363636363636364}
                  model  score_test  ...  can_infer fit_order
0      RandomForestGini    0.829545  ...       True         5
1      RandomForestEntr    0.825758  ...       True         6
2        ExtraTreesEntr    0.825758  ...       True         9
3        ExtraTreesGini    0.814394  ...       True         8
4         LightGBMLarge    0.810606  ...       True        13
5               XGBoost    0.795455  ...       True        11
6              LightGBM    0.791667  ...       True         4
7            LightGBMXT    0.787879  ...       True         3
8        NeuralNetTorch    0.784091  ...       True        12
9              CatBoost    0.776515  ...       True         7
10  WeightedEnsemble_L2    0.776515  ...       True        14
11      NeuralNetFastAI    0.768939  ...       True        10
12       KNeighborsDist    0.681818  ...       True         2
13       KNeighborsUnif    0.674242  ...       True         1

[14 rows x 13 columns]



(automl)  ✘ ddp@dell-Precision-7920-Tower-0  ~/nlp/github/paper/mypaper_code/automl/tasks/titanic  ↱ main ±  python -u ag_titanic.py  > /home/ddp/nlp/github/paper/mypaper_code/automl/logs/ag_titanic.log
No path specified. Models will be saved in: "AutogluonModels/ag-20240306_095933"
No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.
        Recommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):
        presets='best_quality'   : Maximize accuracy. Default time_limit=3600.
        presets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.
        presets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.
        presets='medium_quality' : Fast training time, ideal for initial prototyping.
Beginning AutoGluon training ...
AutoGluon will save models to "AutogluonModels/ag-20240306_095933"
=================== System Info ===================
AutoGluon Version:  1.0.0
Python Version:     3.10.13
Operating System:   Linux
Platform Machine:   x86_64
Platform Version:   #63~20.04.1-Ubuntu SMP Wed Nov 30 13:40:16 UTC 2022
CPU Count:          40
Memory Avail:       60.51 GB / 125.55 GB (48.2%)
Disk Space Avail:   22.68 GB / 915.32 GB (2.5%)
===================================================
Train Data Rows:    627
Train Data Columns: 9
Label Column:       survived
AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).
        2 unique label values:  [0, 1]
        If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])
Problem Type:       binary
Preprocessing data ...
Selected class <--> label mapping:  class 1 = 1, class 0 = 0
Using Feature Generators to preprocess the data ...
Fitting AutoMLPipelineFeatureGenerator...
        Available Memory:                    61960.14 MB
        Train Data (Original)  Memory Usage: 0.21 MB (0.0% of available memory)
        Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.
        Stage 1 Generators:
                Fitting AsTypeFeatureGenerator...
                        Note: Converting 2 features to boolean dtype as they only contain 2 unique values.
        Stage 2 Generators:
                Fitting FillNaFeatureGenerator...
        Stage 3 Generators:
                Fitting IdentityFeatureGenerator...
                Fitting CategoryFeatureGenerator...
                        Fitting CategoryMemoryMinimizeFeatureGenerator...
        Stage 4 Generators:
                Fitting DropUniqueFeatureGenerator...
        Stage 5 Generators:
                Fitting DropDuplicatesFeatureGenerator...
        Types of features in original data (raw dtype, special dtypes):
                ('float', [])  : 2 | ['age', 'fare']
                ('int', [])    : 2 | ['n_siblings_spouses', 'parch']
                ('object', []) : 5 | ['sex', 'class', 'deck', 'embark_town', 'alone']
        Types of features in processed data (raw dtype, special dtypes):
                ('category', [])  : 3 | ['class', 'deck', 'embark_town']
                ('float', [])     : 2 | ['age', 'fare']
                ('int', [])       : 2 | ['n_siblings_spouses', 'parch']
                ('int', ['bool']) : 2 | ['sex', 'alone']
        0.0s = Fit runtime
        9 features in original data used to generate 9 features in processed data.
        Train Data (Processed) Memory Usage: 0.02 MB (0.0% of available memory)
Data preprocessing and feature engineering runtime = 0.05s ...
AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'
        To change this, specify the eval_metric parameter of Predictor()
Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 501, Val Rows: 126
User-specified model hyperparameters to be fit:
{
        'NN_TORCH': {},
        'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],
        'CAT': {},
        'XGB': {},
        'FASTAI': {},
        'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],
        'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],
        'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],
}
Fitting 13 L1 models ...
Fitting model: KNeighborsUnif ...
        0.6349   = Validation score   (accuracy)
        0.05s    = Training   runtime
        0.04s    = Validation runtime
Fitting model: KNeighborsDist ...
        0.6429   = Validation score   (accuracy)
        0.04s    = Training   runtime
        0.05s    = Validation runtime
Fitting model: LightGBMXT ...
        0.8333   = Validation score   (accuracy)
        0.41s    = Training   runtime
        0.02s    = Validation runtime
Fitting model: LightGBM ...
        0.8571   = Validation score   (accuracy)
        0.54s    = Training   runtime
        0.02s    = Validation runtime
Fitting model: RandomForestGini ...
        0.8254   = Validation score   (accuracy)
        1.87s    = Training   runtime
        0.11s    = Validation runtime
Fitting model: RandomForestEntr ...
        0.8175   = Validation score   (accuracy)
        1.83s    = Training   runtime
        0.13s    = Validation runtime
Fitting model: CatBoost ...
        0.8651   = Validation score   (accuracy)
        0.98s    = Training   runtime
        0.0s     = Validation runtime
Fitting model: ExtraTreesGini ...
        0.8254   = Validation score   (accuracy)
        1.85s    = Training   runtime
        0.12s    = Validation runtime
Fitting model: ExtraTreesEntr ...
        0.8254   = Validation score   (accuracy)
        1.76s    = Training   runtime
        0.12s    = Validation runtime
Fitting model: NeuralNetFastAI ...
No improvement since epoch 4: early stopping
        0.8571   = Validation score   (accuracy)
        2.42s    = Training   runtime
        0.01s    = Validation runtime
Fitting model: XGBoost ...
        0.8333   = Validation score   (accuracy)
        0.28s    = Training   runtime
        0.01s    = Validation runtime
Fitting model: NeuralNetTorch ...
        0.8095   = Validation score   (accuracy)
        2.24s    = Training   runtime
        0.02s    = Validation runtime
Fitting model: LightGBMLarge ...
        0.8413   = Validation score   (accuracy)
        1.48s    = Training   runtime
        0.03s    = Validation runtime
Fitting model: WeightedEnsemble_L2 ...
        Ensemble Weights: {'CatBoost': 1.0}
        0.8651   = Validation score   (accuracy)
        0.74s    = Training   runtime
        0.0s     = Validation runtime
AutoGluon training complete, total runtime = 17.44s ... Best model: "WeightedEnsemble_L2"
TabularPredictor saved. To load, use: predictor = TabularPredictor.load("AutogluonModels/ag-20240306_095933")
Loaded data from: /home/ddp/nlp/github/paper/mypaper_code/automl/data/titanic/test.csv | Columns = 10 / 10 | Rows = 264 -> 264